---
title: "Final Proyect"
author: "Liam Montero Guillemi"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: yes
    toc_depth: 2
---

\newpage

# **Introduction**

## **What will the project do?**
In this project, we want to create a movie recommendation system, in which we will use RMSE as a measure of loss.

## **Why is a good recommendation system important?**
A good recommendation system is crucial because it allows new users to discover personalized movies from the start, increasing their satisfaction with the platform. It facilitates the discovery of content beyond what is popular, exposing users to options that truly interest them. This not only improves the user experience but also plays a fundamental role in long-term retention by offering continuous value through relevant suggestions. Ultimately, an effective system is key to the platform's success and competitiveness.

## **How does it benefit businesses and users?**
A good recommendation system benefits both businesses and users. For businesses, it boosts retention, increases revenue, provides valuable user data, and offers a competitive advantage. For users, it facilitates personalized movie discovery, reduces information overload, exposes them to new content, and improves the overall platform experience, saving them time and increasing satisfaction. In essence, it creates a mutually beneficial relationship by connecting users with the content they love and businesses with more engaged audiences.

\newpage

# **Creating the dataset**

This part of the project was provided by the Edx platform, below we will explain step by step each line of it.
The following lines of code install the tidyverse and caret packages if they aren't already installed. They are then loaded using the library() function.

```{r message = FALSE} 
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
```

Then we have the following 3 lines of code, which perform the following actions:
  1) download the 10 million data document from the MovieLens dataset
  2) extract the ratings data from the downloaded file
  3) extract the movie name data

```{r}
dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)
```

After that, the data is converted into a format that is easy to work with, in this case it is a data frame, everything that is inside the ratings_file file is read, then each of its rows is divided into columns using the separator “::”

```{r}
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
```

then each of these columns is named as follows:

```{r}
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
```

The data frame is configured so that each of the columns has the data in the desired format:

```{r}
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))
```

We do the same thing as we did with the ratings_file, but this time with the movies_file, that is, we convert the data to an easy-to-work format, name its columns, and configure the data to a desired format:

```{r}
movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)

colnames(movies) <- c("movieId", "title", "genres")

movies <- movies %>%
  mutate(movieId = as.integer(movieId))
```

The 2 data frames created previously are joined using the movieId column:

```{r}
movielens <- left_join(ratings, movies, by = "movieId")
```

Now, we proceed to divide the data set into a training set and a test set:

```{r message = FALSE, warning = FALSE}
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
```

We make sure that all data in the test set is also in the training set as follows:

```{r}
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

```

The rows that were removed from the test set are added to the training set so that this information is not lost:

```{r results = 'hide', message = FALSE, warning = FALSE}
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)
```

Finally, all objects created to shape the training and test sets but no longer used are deleted from the workspace. This is done to free up memory space and allow for more comfortable work.

```{r}
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

\newpage

# Data Exploration

In this section an overview over the edx dataset is made. In Table 1 and Table 2 the number of rows and the structure of the data in the first 10 rows can be seen.

```{r number_rows, eval=TRUE, cache=TRUE, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
if(!require(tibble)) install.packages("tibble")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(lubridate)) install.packages("lubridate")
if(!require(ggthemes)) install.packages("ggthemes")
if(!require(scales)) install.packages("scales")
if(!require(knitr)) install.packages("knitr")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(dslabs)) install.packages("dslabs")
if(!require(gam)) install.packages("gam")
if(!require(gridExtra)) install.packages("gridExtra")

library(tibble)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(scales)
library(knitr)
library(kableExtra)


# Set a seed for reproducibility
set.seed(1)

# Check the number of rows in the reduced dataset
data.frame(Rows = nrow(edx))
```

```{r first_rows, eval=TRUE, cache=TRUE, include=TRUE, echo=FALSE}
# Show head of dataset edx (first 10 rows)
head(edx, 10)

```


## Ratings

In the data, movie ratings are included.  Possible ratings were in the range between 0.5 and 5.  Hereby, whole numbers seem to be preferred.  The distribution of the number of ratings on the different rating values can be seen in Figure 1.  In Table 3, the top 10 films with the most ratings are listed.

```{r distribution_ratings, eval=TRUE, cache=TRUE, include=TRUE, echo=FALSE, fig.cap="Distribution of film ratings"}
# Plot distribution of film ratings in edx
ggplot(edx, aes(x = rating)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  scale_x_continuous(breaks = seq(min(edx$rating)-0.5, max(edx$rating), by = 0.5)) +
  scale_y_continuous(breaks = seq(0, max(length(edx$rating)), by = 500000),labels = function(x) format(x, scientific = FALSE,big.mark=",")) +
  labs(x = "Rating", y = "Number of ratings", caption = "Source: edx data")
```

```{r Top10, eval=TRUE, include=TRUE, echo=FALSE, cache=TRUE}
library(dplyr)

# Plot Top 10 Films with the most ratings in a table
edx %>%
  group_by(movieId, title) %>%
  summarise(count = n(), .groups = 'drop') %>%
  arrange(desc(count)) %>%
  head(10)
```

\newpage

The dataset includes ratings from 1995 to 2009. During that time, the number of ratings made changes. In Figure 2, the number of ratings are summed up per week and plotted by time. It can be seen that before 2000, the number of ratings varied a lot. After 2000, the number of ratings reached a certain level with some peaks, decreased again but stayed at a level of about 22,000 ratings per week till 2009. The peaks may possibly come from a blockbuster movie coming out at that time.

```{r Ratings_over_time, eval=TRUE, include=TRUE, echo=FALSE, cache=TRUE, fig.cap="Number of ratings over time (weekly)"}
# Convert timestamp to Date format for plotting
edx$date <- as.Date(as.POSIXct(edx$timestamp, origin = "1970-01-01"))

# Create a line plot showing the number of ratings per week
ggplot(edx, aes(x = floor_date(as.Date(as.POSIXct(timestamp, origin = "1970-01-01")), "week"))) +
  geom_line(stat = "count", color = "darkgreen") +
  labs(x = "Year", 
       y = "Number of ratings", 
       caption = "Source: edx data") +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(trans = "sqrt", labels = function(x) format(x, scientific = FALSE,big.mark=","))
```


## Related observations

The next two diagrams in Figure 4 show the average rating vs. number of users, respectively, the average number of ratings per user.
Most of the users have an average rating overall of 3.5 - 3.8. This seems to follow a Gaussian distribution. On the right, it can be seen that there are some "power users" (around 200 - 400) who have several thousand ratings. However, most of the users only made a small number of ratings.

```{r user_prep, eval=TRUE, echo=FALSE, cache=TRUE, include=FALSE}
# user_prep: Preparing data by extracting release year from movie titles
edx$released <- str_extract(edx$title, "\\((\\d{4})\\)") %>% 
                                  str_remove_all("[()]") %>% 
                                  as.numeric()
```

```{r user_plots, eval=TRUE, echo=FALSE, cache=TRUE, include=TRUE, fig.width=6.6, fig.height=3, fig.cap="Average rating per user (left) and average number of ratings per user (right)"}
# user_plots: Creating two side-by-side histograms for average rating per user and number of ratings per user
library(gridExtra)

grid.arrange(

# Plot 1: Histogram of average rating per user
edx %>% group_by(userId) %>%
  summarise(ave_rating = sum(rating)/n()) %>%
  ggplot(aes(ave_rating)) +
  geom_histogram(bins=30, fill = "skyblue", color = "black") +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE,big.mark=",")) +
  labs(x = "Average rating", y = "Number of users", caption = "Source: edx data"),

# Plot 2: Histogram of number of ratings per user
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins=30, fill = "skyblue", color = "black") +
  scale_x_log10(labels = function(x) format(x, scientific = FALSE,big.mark=",")) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE,big.mark=",")) +
  labs(x = "Users", y = "Number of ratings", caption = "Source: edx data"),
  
  # Arrange plots in 2 columns and set width  
  ncol = 2, 
  widths = c(1, 1)
)
```


## Time dependent observations

In the dataset, there are time dimensions that should be mentioned. The first one is the time of rating. Possibly, in some time period or time of the year, the average ratings are different. So, there could possibly be a time effect on the rating value. 
The second time dimension is the release year of the specific movie. This value can be extracted from the title in the dataset. If the average rating per release year of the film is plotted in a diagram, it can be seen that older films tend to have a higher rating than newer films. 
So, both effects could play a role in developing the algorithm to predict the ratings. The two diagrams can be seen in Figure 5.

```{r time_dependent_plots, eval=TRUE, echo=FALSE, cache=TRUE, include=TRUE, fig.width=6.6, fig.height=3, fig.cap="Date vs. weekly averaged rating (left) and release year vs. averaged rating (right)", warning=FALSE, message=FALSE}
# time_dependent_plots: Plot weekly averaged rating vs. date and averaged rating vs. release year
library(gridExtra)

grid.arrange(
  # Scatter plot of weekly averaged rating over time
  edx %>% 
    ggplot(aes(date, rating)) +
    geom_point(data = edx %>% 
               mutate(date = round_date(date, unit = "week")) %>%
               group_by(date) %>%
               summarize(rating = mean(rating)),
             aes(x = date, y = rating), 
             alpha = 0.5, color = "black", size = 0.7) +
    labs(x = "Date", y = "Rating", caption = "Source: edx data") +
    coord_cartesian(ylim = c(3, 4)),
  
  # Scatter plot of averaged rating by release year
  edx %>%
    group_by(released) %>%
    summarize(rating = mean(rating)) %>%
    ggplot(aes(released, rating)) +
    geom_point(size = 0.7) +
    labs(x = "Released", y = "Rating", caption = "Source: edx data"),
  
  ncol = 2, 
  widths = c(1, 1)
)
```

\newpage


# **Creating the model**

## **How did I find which model to use?**

I'd like to start by saying that I decided to use the matrix factorization method because it offers the best results on large datasets.
I decided to use the recosystem package and not the recommenderlab package because the recosystem package is optimized for large datasets, while the recommenderlab package is not. I learned this through trial and error. With the recommenderlab package, my computer never managed to finish the model; it always shut down before finishing it. I spent many hours running calculations. I think an Intel Core i9-13980HX isn't a bad processor, and paired with 32GB of RAM, it's not bad at all.
Therefore, I decided to change my approach and use recosystem, which is designed to work with large datasets. It can perform parallel operations using the CPU, which greatly shortens processing times. It can load the training and prediction sets from the hard drive as a file, which means it doesn't overload the RAM too much. Although it also has the option to load them from the R workspace.

## **Implementation of the model**

The first thing I did was install and load recosystem

```{r results = 'hide', message = FALSE, warning = FALSE}
if(!require(recosystem)) install.packages("recosystem")
if(!require(parallel)) install.packages("parallel")
library(recosystem)
library(parallel)
```

Then, we adjust the training set with only the parameters we are going to use, this is not absolutely necessary, but I like to do it to be more organized.

```{r}
train_set <- edx %>% select(userId, movieId, rating) %>% data.frame()
```

We convert the training set into a format that the recosystem ecosystem can interpret, for this we use the data_memory() function, which uses an object that is in the R workspace and converts it into a DataSource object, which is the type of object used to train and predict models with the recosystem library.

```{r}
train_data <- data_memory(train_set[, 1], train_set[, 2], rating = train_set[, 3])
```

creation of a RecoSys object, which will contain all the parameters, training and predictions.

```{r}
r <- Reco()
```

Next, a hyperparameter adjustment is made, this is carried out by the r$tune function, which has a large list of many parameters to adjust, the vast majority of the parameters I chose were those that came by default, this because with chatGPT I did a search for the best parameters for large data sets and this gave me an answer that was very similar to the parameters that came by default. In the parameter nthread = detectCores() - 1, I put it like this because what this parameter does is use all the cores of your CPU except 1, and the parameter nbin = 32, is because it always has to be greater than the nthread parameter, so, since my CPU has 32 cores, nthread = 31 and nbin = 32.

This function performs cross-validation, which has 5 folds by default (which can also be adjusted), and runs all possible combinations of all parameters to find the best combination of them that results in a lower RMSE, if I wanted to evaluate using another loss function, I could also do so by adjusting the loss parameter, which has the RMSE by default.

```{r results = 'hide', message = FALSE, warning = FALSE}
best_tune <- r$tune(train_data, opts = list(dim = c(10L, 20L, 30L),
                               costp_l1 = c(0, 0.1),
                               costp_l2 = c(0.01, 0.1),
                               costq_l1 = c(0, 0.1),
                               costq_l2 = c(0.01, 0.1),
                               lrate    = c(0.01, 0.1),
                               nthread  = detectCores() - 1, 
                               nbin     = detectCores())
)
```

Here I proceed to train the recommendation model with r$train, using the best parameters found with the r$tune function

```{r results = 'hide'}
final_model <- r$train(train_data, opts = best_tune$min)
```

Then, we proceed to prepare the evaluation data set, just as we did with the test data set, but this time we do not include the ratings, because that is what I am supposed to predict.

```{r}
test_data <- data_memory(final_holdout_test[, 1], final_holdout_test[, 2])
```

We make predictions with r$predict and the object obtained is a vector, which contains all the ratings predicted by the model for the users.

```{r}
pred2 = r$predict(test_data, out_memory())
```

Finally, we proceed to calculate the RMSE, to see the accuracy of our model, we do this using the vector of real values of the evaluation set and the vector obtained from the model's prediction on the evaluation set.
General form of the least squares loss formula:

$RMSE <- sqrt(mean((true.values - predict.values)^2))$

```{r}
RMSE <- sqrt(mean((final_holdout_test[, 3] - pred2)^2))
RMSE
```

\newpage

# **Conclusions**

This project has enabled the development of a movie recommendation system based on matrix factorization techniques, demonstrating the efficiency of machine learning methods applied to large datasets. The most important points are highlighted below:

• **Data Preprocessing and Organization:**
A rigorous extraction and transformation process was implemented for the Movielens dataset, ensuring proper data structuring from files with millions of records. The division into training and test sets, along with data integrity validation, was essential to build a solid foundation on which to train and evaluate the model.

• **Choice of Tools and Methodology:**
The decision to use the recosystem package instead of recommenderlab was based on the need to handle a large volume of data and on computational efficiency. Recosystem enabled hyperparameter optimization through cross-validation and leveraged parallelization, resulting in a significant reduction in processing times and more efficient management of hardware resources. 

• **Reflections and Future Directions:**
This project demonstrates the importance of integrating data science techniques with robust optimization methodologies for handling large volumes of data. It also lays the groundwork for future improvements, such as the incorporation of new algorithms, further hyperparameter optimization, or adaptation of the system to other domains. The experience gained demonstrates that the combination of good preprocessing, the choice of appropriate tools, and an iterative approach to validation can lead to highly effective solutions in real-world environments.